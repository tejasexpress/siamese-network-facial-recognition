{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ff0527",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7231e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('D:/Anaconda/envs/task1/Lib/site-packages')\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1debc18b",
   "metadata": {},
   "source": [
    "# Setting GPU consumption growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dcf25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ae0b5",
   "metadata": {},
   "source": [
    "# Creating paths for Positive, Negative and Anchor Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_PATH = os.path.join('data','positive')\n",
    "NEG_PATH = os.path.join('data','negative')\n",
    "ANC_PATH = os.path.join('data','anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f732c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make directories\n",
    "os.makedirs(POS_PATH)\n",
    "os.makedirs(NEG_PATH)\n",
    "os.makedirs(ANC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7bc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in os.listdir('lfw'):\n",
    "    for file in os.listdir(os.path.join('lfw',directory)):\n",
    "        EX_PATH = os.path.join('lfw',directory,file)\n",
    "        NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "        os.replace(EX_PATH,NEW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a950bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import uuid library for unique image names\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd596c",
   "metadata": {},
   "source": [
    "# video capture to gather positive and anchor Images\n",
    "\n",
    "* we gather negative images from the 'lone faces in the wild' dataset but for anchor and positive images make sure the dataset has images with varying backgrounds and lightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#capture video and show\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = frame[120:370, 200:450, :]\n",
    "    cv2.imshow('Image Collection', frame)\n",
    "    \n",
    "    #collect anchors\n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        imgname = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname,frame)\n",
    "    \n",
    "    #collect positives\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        imgname = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname,frame)\n",
    "        \n",
    "       \n",
    "    #breaking\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d6201",
   "metadata": {},
   "source": [
    "# take 400 photos each for training the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91dc54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = tf.data.Dataset.list_files(ANC_PATH+'\\*jpg').take(400)\n",
    "positive = tf.data.Dataset.list_files(POS_PATH+'\\*jpg').take(400)\n",
    "negative = tf.data.Dataset.list_files(NEG_PATH+'\\*jpg').take(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6bd7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image scaling from [0,1]\n",
    "def preprocess(file_path):\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    img = tf.image.resize(img, (100,100))\n",
    "    img = img / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcaf4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create labelled dataset\n",
    "positives = tf.data.Dataset.zip((anchor,positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor,negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a38efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85604199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    return(preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b8b22a",
   "metadata": {},
   "source": [
    "# Dataloader Pipeline\n",
    "* zip data into labelled dataset with each examples containing an anchor img, positive/negative img, and a 0/1 label\n",
    "* concatenate into one data\n",
    "* preprocess the images inside every example\n",
    "* shuffle and take 70% as training data\n",
    "* batch it into 16 per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e21b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training partition\n",
    "train_data = data.take(round(len(data)*0.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing partition\n",
    "test_data = data.skip(round(len(data)*0.7))\n",
    "test_data = test_data.take(round(len(data)*0.3))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04abddd",
   "metadata": {},
   "source": [
    "# embedding layer\n",
    "* contains the structure of the neural network\n",
    "* will take a 100x100 input\n",
    "* pass through a 2D convolution layer -> maxpooling layer three times\n",
    "* flatten the feature map\n",
    "* pass through a sigmoid dense layer with 4096 features nodes each of which is connected to every node of the flatten layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ada1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding():\n",
    "    inp = Input(shape=(100,100,3), name=\"input_image\")\n",
    "    \n",
    "    #first block\n",
    "    c1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
    "    m1 = MaxPooling2D(64, (2,2), padding ='same')(c1)\n",
    "    \n",
    "    #second block\n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64, (2,2), padding ='same')(c2) \n",
    "               \n",
    "    #third block\n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64, (2,2), padding ='same')(c3)\n",
    "    \n",
    "    #final embedding\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation = 'sigmoid')(f1) \n",
    "    \n",
    "    return Model(inputs=[inp], outputs=[d1], name='embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4efc0a4",
   "metadata": {},
   "source": [
    "# Siamese distance layer\n",
    "to calculate the distance between the anchor image and the corresponding positive/negative image after it has passed through the  embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2851436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74afd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = make_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10736ca1",
   "metadata": {},
   "source": [
    "# the final model\n",
    "\n",
    "* takes the input and validation image\n",
    "* passess both through the embedding\n",
    "* calculates the siamese distance between the both dense layers\n",
    "* uses this distance to create a dense classifier with sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba82e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "    \n",
    "    #inputs\n",
    "    input_image = Input(name='input_img', shape=(100,100,3))\n",
    "    validation_image = Input(name='validation_img', shape=(100,100,3))\n",
    "    \n",
    "    #siamese distance\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "    \n",
    "    #classification layer\n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff56c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f386ccc1",
   "metadata": {},
   "source": [
    "# we use cross entropy as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb26db",
   "metadata": {},
   "outputs": [],
   "source": [
    " #loss function\n",
    "binary_cross_loss = tf.losses.BinaryCrossentropy()\n",
    " #optimiser\n",
    "opt = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model = siamese_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9460aa",
   "metadata": {},
   "source": [
    "# Defining each training step\n",
    "* get a batch from the train set\n",
    "* get anchor and validation images as x\n",
    "* get label as y\n",
    "* pass x into the neural net to get classifier value yhat\n",
    "* calculate loss between y and yhat\n",
    "* calculate gradients of loss function with respect to all trainable variables inside the model\n",
    "* apply the gradients to all trainable variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        #get anchor and positive/negative image\n",
    "        x = batch[:2]\n",
    "        #get label\n",
    "        y = batch[2]\n",
    "        \n",
    "        #forward pass\n",
    "        yhat = siamese_model(x, training=True)\n",
    "        #calculate loss\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "        \n",
    "    #gradients\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    \n",
    "    #backprop\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "     \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3693789",
   "metadata": {},
   "source": [
    "# train the data in batches and save every 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, EPOCHS):\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch,EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        for idx, batch in enumerate(data):\n",
    "            train_step(batch)\n",
    "            progbar.update(idx+1)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_data, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa0c9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save weights\n",
    "\n",
    "siamese_model.save('siamesemodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe115d2",
   "metadata": {},
   "outputs": [],
   "source": [
    " #reload model\n",
    "model = tf.keras.models.load_model('siamesemodel.h5', \n",
    "                                    custom_objects={'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89acb57",
   "metadata": {},
   "source": [
    "# verification function for few shot face verification after training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(frame, model, detection_threshold, verification_threshold):\n",
    "    results = []\n",
    "    for image in os.listdir(os.path.join('application_data','verification_images')):\n",
    "        input_img = preprocess(os.path.join('application_data','input_image','input_image.jpg'))\n",
    "        validation_img = preprocess(os.path.join('application_data','verification_images',image))\n",
    "        \n",
    "        result = model.predict(list(np.expand_dims([input_img,validation_img], axis=1)))\n",
    "        results.append(result)\n",
    "        \n",
    "    detection = np.sum(np.array(results) > detection_threshold)\n",
    "    verification = detection / len(os.listdir(os.path.join('application_data', 'verification_images')))\n",
    "    verified = verification > verification_threshold\n",
    "    \n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37faa328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opencv detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = frame[120:370, 200:450, :]\n",
    "    \n",
    "    cv2.imshow('verification', frame)\n",
    "    \n",
    "    #verification\n",
    "    if cv2.waitKey(10) & 0xFF == ord('v'):\n",
    "        cv2.imwrite(os.path.join('application_data','input_image','input_image.jpg'), frame)\n",
    "        results, verified = verify(frame, model, 0.6, 0.7)\n",
    "        print(verified)\n",
    "        \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af61faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35297ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1968f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.squeeze(results) >0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea1432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
